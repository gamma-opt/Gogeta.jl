{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Example: Neural Networks – Relaxing walk"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Before runnning this notebook, make sure that all neccesary libraries are installed:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "using Flux\n",
    "using Random\n",
    "using Gogeta\n",
    "using Gurobi\n",
    "using JuMP\n",
    "using Plots\n",
    "using Revise\n",
    "using QuasiMonteCarlo"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this example, we are going to introduce the function `optimize_by_walking!()` that can be used to optimize MILP formulation of the neural network faster than optimizing the formulation directly. To understand the method in detail, see Tong, J et al. (2024)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Innitialize neural network with random weights"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Chain(\n",
       "  Dense(2 => 100, relu),                \u001b[90m# 300 parameters\u001b[39m\n",
       "  Dense(100 => 100, relu),              \u001b[90m# 10_100 parameters\u001b[39m\n",
       "  Dense(100 => 1),                      \u001b[90m# 101 parameters\u001b[39m\n",
       ") \u001b[90m                  # Total: 6 arrays, \u001b[39m10_501 parameters, 41.395 KiB."
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dimension = 2\n",
    "\n",
    "begin\n",
    "    Random.seed!(12345);\n",
    "\n",
    "    NN_model = Chain(\n",
    "        Dense(dimension => 100, relu),\n",
    "        Dense(100 => 100, relu),\n",
    "        Dense(100 => 1)\n",
    "    )\n",
    "end"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Formulate  NN as a MILP "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We set up upper and lower bounds for the variables in which our MILP formulation is guranteed to output the same values as the original NN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "init_U = [5.0, 5.0];\n",
    "init_L = [-5.0, -5.0];"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Formulate MILP model with fast bound tightening. Set objective funciton of the model as a maximiztion of the output neuron."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Set parameter Username\n",
      "Academic license - for non-commercial use only - expires 2025-05-20\n"
     ]
    },
    {
     "data": {
      "text/latex": [
       "$ x_{3,1} $"
      ],
      "text/plain": [
       "x[3,1]"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Formulate the MIP with heuristic bound tightening\n",
    "jump_model = Model(Gurobi.Optimizer)\n",
    "set_silent(jump_model)\n",
    "NN_formulate!(jump_model, NN_model, init_U, init_L; bound_tightening=\"fast\");\n",
    "\n",
    "last_layer, _ = maximum(keys(jump_model[:x].data))\n",
    "@objective(jump_model, Max, jump_model[:x][last_layer, 1])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "`set_solver!()` should be specified"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "set_solver! (generic function with 1 method)"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "function set_solver!(jump)\n",
    "    set_optimizer(jump, Gurobi.Optimizer)\n",
    "    set_silent(jump)\n",
    "end"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Optimize by relax walking"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In order to use optimization using relax walking, you just need to call function `optimize_by_walking!()` with the following input parameters:\n",
    "- jump_model – empty jump model\n",
    "- NN_model – the neural net that you want to represent as a jump model\n",
    "- init_U, init_L – upper and lower bounds in which our solution is guranteed to generate the same result as NN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Set parameter Username\n",
      "Academic license - for non-commercial use only - expires 2025-05-20\n"
     ]
    },
    {
     "ename": "LoadError",
     "evalue": "MethodError: no method matching local_search(::Vector{Float64}, ::Model, ::Chain{Tuple{Dense{typeof(relu), Matrix{Float32}, Vector{Float32}}, Dense{typeof(relu), Matrix{Float32}, Vector{Float32}}, Dense{typeof(identity), Matrix{Float32}, Vector{Float32}}}}, ::Vector{Float64}, ::Vector{Float64})\n\n\u001b[0mClosest candidates are:\n\u001b[0m  local_search(::Any, ::Any, ::Any, ::Any; max_iter, epsilon, show_path, logging, tolerance)\n\u001b[0m\u001b[90m   @\u001b[39m \u001b[35mGogeta\u001b[39m \u001b[90m~/.julia/packages/Gogeta/iJMCg/src/neural_networks/\u001b[39m\u001b[90m\u001b[4mrelaxing_walk.jl:171\u001b[24m\u001b[39m\n",
     "output_type": "error",
     "traceback": [
      "MethodError: no method matching local_search(::Vector{Float64}, ::Model, ::Chain{Tuple{Dense{typeof(relu), Matrix{Float32}, Vector{Float32}}, Dense{typeof(relu), Matrix{Float32}, Vector{Float32}}, Dense{typeof(identity), Matrix{Float32}, Vector{Float32}}}}, ::Vector{Float64}, ::Vector{Float64})\n\n\u001b[0mClosest candidates are:\n\u001b[0m  local_search(::Any, ::Any, ::Any, ::Any; max_iter, epsilon, show_path, logging, tolerance)\n\u001b[0m\u001b[90m   @\u001b[39m \u001b[35mGogeta\u001b[39m \u001b[90m~/.julia/packages/Gogeta/iJMCg/src/neural_networks/\u001b[39m\u001b[90m\u001b[4mrelaxing_walk.jl:171\u001b[24m\u001b[39m\n",
      "",
      "Stacktrace:",
      " [1] optimize_by_walking!(original::Model, nn_model::Chain{Tuple{Dense{typeof(relu), Matrix{Float32}, Vector{Float32}}, Dense{typeof(relu), Matrix{Float32}, Vector{Float32}}, Dense{typeof(identity), Matrix{Float32}, Vector{Float32}}}}, U_in::Vector{Float64}, L_in::Vector{Float64}; delta::Float64, return_sampled::Bool, logging::Bool, iterations::Int64, infeasible_per_iter::Int64)",
      "   @ Gogeta ~/.julia/packages/Gogeta/iJMCg/src/neural_networks/relaxing_walk.jl:44",
      " [2] optimize_by_walking!(original::Model, nn_model::Chain{Tuple{Dense{typeof(relu), Matrix{Float32}, Vector{Float32}}, Dense{typeof(relu), Matrix{Float32}, Vector{Float32}}, Dense{typeof(identity), Matrix{Float32}, Vector{Float32}}}}, U_in::Vector{Float64}, L_in::Vector{Float64})",
      "   @ Gogeta ~/.julia/packages/Gogeta/iJMCg/src/neural_networks/relaxing_walk.jl:18",
      " [3] top-level scope",
      "   @ In[18]:1"
     ]
    }
   ],
   "source": [
    "x_opt, opt = optimize_by_walking!(jump_model, NN_model, init_U, init_L)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It returns optimal solution along with the optimal objective function."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Julia 1.10.0",
   "language": "julia",
   "name": "julia-1.10"
  },
  "language_info": {
   "file_extension": ".jl",
   "mimetype": "application/julia",
   "name": "julia",
   "version": "1.10.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
