var documenterSearchIndex = {"docs":
[{"location":"api/#Public-API","page":"Public API","title":"Public API","text":"","category":"section"},{"location":"api/","page":"Public API","title":"Public API","text":"These are all of the functions and data structures that the user needs to know in order to use this package.","category":"page"},{"location":"api/#Tree-ensembles","page":"Public API","title":"Tree ensembles","text":"","category":"section"},{"location":"api/#Data-structures","page":"Public API","title":"Data structures","text":"","category":"section"},{"location":"api/","page":"Public API","title":"Public API","text":"TEModel - holds the parameters from a tree ensemble model","category":"page"},{"location":"api/#Tree-model-parameter-extraction","page":"Public API","title":"Tree model parameter extraction","text":"","category":"section"},{"location":"api/","page":"Public API","title":"Public API","text":"extract_evotrees_info - get the necessary parameters from an EvoTrees model","category":"page"},{"location":"api/#MIP-formulation","page":"Public API","title":"MIP formulation","text":"","category":"section"},{"location":"api/","page":"Public API","title":"Public API","text":"TE_formulate! - formulate a JuMP model from a tree ensemble without the split constraints","category":"page"},{"location":"api/#Input-optimization","page":"Public API","title":"Input optimization","text":"","category":"section"},{"location":"api/","page":"Public API","title":"Public API","text":"add_split_constraints! - add all split constraints to the formulation\ntree_callback_algorithm - used to add only necessary split constraints during callbacks","category":"page"},{"location":"api/#Showing-the-solution","page":"Public API","title":"Showing the solution","text":"","category":"section"},{"location":"api/","page":"Public API","title":"Public API","text":"get_solution - get human-readable solution in the form of upper and lower bounds for the input variables","category":"page"},{"location":"api/#Neural-networks","page":"Public API","title":"Neural networks","text":"","category":"section"},{"location":"api/#MIP-formulation-2","page":"Public API","title":"MIP formulation","text":"","category":"section"},{"location":"api/","page":"Public API","title":"Public API","text":"NN_formulate! - formulate a JuMP model, perform simultaneous bound tightening and possibly compression","category":"page"},{"location":"api/#Compression","page":"Public API","title":"Compression","text":"","category":"section"},{"location":"api/","page":"Public API","title":"Public API","text":"NN_compress - compress a neural network using precomputed activation bounds","category":"page"},{"location":"api/#Forward-pass","page":"Public API","title":"Forward pass","text":"","category":"section"},{"location":"api/","page":"Public API","title":"Public API","text":"forward_pass! - fix the input variables and optimize the model to get the output","category":"page"},{"location":"api/#Sampling-based-optimization","page":"Public API","title":"Sampling-based optimization","text":"","category":"section"},{"location":"api/","page":"Public API","title":"Public API","text":"optimize_by_sampling! - optimize the JuMP model by using a sampling-based approach\noptimize_by_walking! - optimize the JuMP model by using a more sophisticated sampling-based approach","category":"page"},{"location":"api/#Convolutional-neural-networks","page":"Public API","title":"Convolutional neural networks","text":"","category":"section"},{"location":"api/#Data-structures-2","page":"Public API","title":"Data structures","text":"","category":"section"},{"location":"api/","page":"Public API","title":"Public API","text":"CNNStructure - container for the layer stucture of a convolutional neural network model","category":"page"},{"location":"api/#Parameter-extraction","page":"Public API","title":"Parameter extraction","text":"","category":"section"},{"location":"api/","page":"Public API","title":"Public API","text":"get_structure - get layer structure from a convolutional neural network model","category":"page"},{"location":"api/#MIP-formulation-3","page":"Public API","title":"MIP formulation","text":"","category":"section"},{"location":"api/","page":"Public API","title":"Public API","text":"CNN_formulate! - formulate a JuMP model from the CNN","category":"page"},{"location":"api/#Forward-pass-2","page":"Public API","title":"Forward pass","text":"","category":"section"},{"location":"api/","page":"Public API","title":"Public API","text":"image_pass! - fix the input variables and optimize the model to get the ouput","category":"page"},{"location":"api/#Sampling-based-optimization-2","page":"Public API","title":"Sampling-based optimization","text":"","category":"section"},{"location":"api/","page":"Public API","title":"Public API","text":"optimize_by_walking_CNN! - optimize the JuMP model by using a more sophisticated sampling-based approach","category":"page"},{"location":"tree_ensembles/#Tree-ensembles","page":"Tree ensembles","title":"Tree ensembles","text":"","category":"section"},{"location":"tree_ensembles/#Formulation","page":"Tree ensembles","title":"Formulation","text":"","category":"section"},{"location":"tree_ensembles/","page":"Tree ensembles","title":"Tree ensembles","text":"First, one must create and train an EvoTrees tree ensemble model.","category":"page"},{"location":"tree_ensembles/","page":"Tree ensembles","title":"Tree ensembles","text":"using EvoTrees\n\nconfig = EvoTreeRegressor(nrounds=500, max_depth=5)\nevo_model = fit_evotree(config; x_train, y_train)","category":"page"},{"location":"tree_ensembles/","page":"Tree ensembles","title":"Tree ensembles","text":"Then the parameters can be extracted from the trained tree ensemble and used to create a JuMP model containing the tree ensemble MIP formulation.","category":"page"},{"location":"tree_ensembles/","page":"Tree ensembles","title":"Tree ensembles","text":"using Gurobi\nusing Gogeta\n\n# Extract data from EvoTrees model\n\nuniversal_tree_model = extract_evotrees_info(evo_model)\n\n# Create jump model and formulate\njump = Model(() -> Gurobi.Optimizer())\nset_attribute(jump, \"OutputFlag\", 0) # JuMP or solver-specific attributes can be changed\n\nTE_formulate!(jump, universal_tree_model, MIN_SENSE)","category":"page"},{"location":"tree_ensembles/#Optimization","page":"Tree ensembles","title":"Optimization","text":"","category":"section"},{"location":"tree_ensembles/","page":"Tree ensembles","title":"Tree ensembles","text":"There are two ways of optimizing the JuMP model: either by 1) creating the full set of split constraints before optimizing, or 2) using lazy constraints to generate only the necessary ones during the solution process.","category":"page"},{"location":"tree_ensembles/","page":"Tree ensembles","title":"Tree ensembles","text":"1) Full set of constraints","category":"page"},{"location":"tree_ensembles/","page":"Tree ensembles","title":"Tree ensembles","text":"add_split_constraints!(jump, universal_tree_model)\noptimize!(jump)","category":"page"},{"location":"tree_ensembles/","page":"Tree ensembles","title":"Tree ensembles","text":"2) Lazy constraints","category":"page"},{"location":"tree_ensembles/","page":"Tree ensembles","title":"Tree ensembles","text":"# Define a callback function. For each solver this might be slightly different.\n# See JuMP documentation or your solver's Julia interface documentation.\n# Inside the callback 'tree_callback_algorithm' must be called.\n\nfunction split_constraint_callback_gurobi(cb_data, cb_where::Cint)\n\n    # Only run at integer solutions\n    if cb_where != GRB_CB_MIPSOL\n        return\n    end\n\n    Gurobi.load_callback_variable_primal(cb_data, cb_where)\n    tree_callback_algorithm(cb_data, universal_tree_model, jump)\n\nend\n\njump = direct_model(Gurobi.Optimizer())\nTE_formulate!(jump, universal_tree_model, MIN_SENSE)\n\nset_attribute(jump, \"LazyConstraints\", 1)\nset_attribute(jump, Gurobi.CallbackFunction(), split_constraint_callback_gurobi)\n\noptimize!(jump)","category":"page"},{"location":"tree_ensembles/","page":"Tree ensembles","title":"Tree ensembles","text":"The optimal solution (minimum and maximum values for each of the input variables) can be queried after the optimization.","category":"page"},{"location":"tree_ensembles/","page":"Tree ensembles","title":"Tree ensembles","text":"get_solution(opt_model, universal_tree_model)\nobjective_value(opt_model)","category":"page"},{"location":"tree_ensembles/#Recommendations","page":"Tree ensembles","title":"Recommendations","text":"","category":"section"},{"location":"tree_ensembles/","page":"Tree ensembles","title":"Tree ensembles","text":"Using the tree ensemble optimization from this package is quite straightforward. The only parameter the user can change is the solution method: with initial constraints or with lazy constraints. In our computational tests, we have seen that the lazy constraint generation almost invariably produces models that are computationally easier to solve.  Therefore we recommend primarily using it as the solution method, but depending on your use case, trying the initial constraints might also be worthwhile.","category":"page"},{"location":"neural_networks/#Neural-networks","page":"Neural networks","title":"Neural networks","text":"","category":"section"},{"location":"neural_networks/","page":"Neural networks","title":"Neural networks","text":"With neural networks, the hidden layers must use the ReLU activation function, and the output layer must use the identity activation.","category":"page"},{"location":"neural_networks/","page":"Neural networks","title":"Neural networks","text":"A neural networks satifying these requirements can be formulated into a mixed-integer optimization problem.  Along with formulation, the neuron activation bounds can be calculated, which improves computational performance as well as enables compression.","category":"page"},{"location":"neural_networks/","page":"Neural networks","title":"Neural networks","text":"The network is compressed by pruning neurons that are either stabily active or inactive. The activation bounds are used to identify these neurons.","category":"page"},{"location":"neural_networks/#Formulation","page":"Neural networks","title":"Formulation","text":"","category":"section"},{"location":"neural_networks/","page":"Neural networks","title":"Neural networks","text":"First, create a neural network model satisfying the requirements:","category":"page"},{"location":"neural_networks/","page":"Neural networks","title":"Neural networks","text":"using Flux\n\nmodel = Chain(\n    Dense(2 => 10, relu),\n    Dense(10 => 20, relu),\n    Dense(20 => 5, relu),\n    Dense(5 => 1)\n)","category":"page"},{"location":"neural_networks/","page":"Neural networks","title":"Neural networks","text":"Then define the bounds for the input variables. These will be used to calculate the activation bounds for the subsequent layers.","category":"page"},{"location":"neural_networks/","page":"Neural networks","title":"Neural networks","text":"init_U = [-0.5, 0.5];\ninit_L = [-1.5, -0.5];","category":"page"},{"location":"neural_networks/","page":"Neural networks","title":"Neural networks","text":"Now the neural network can be formulated into a MIP. Here optimization-based bound tightening is also used.","category":"page"},{"location":"neural_networks/","page":"Neural networks","title":"Neural networks","text":"jump_model = Model(Gurobi.Optimizer)\nset_silent(model) # set desired parameters\n\nbounds_U, bounds_L = NN_formulate!(jump_model, model, init_U, init_L; bound_tightening=\"standard\")","category":"page"},{"location":"neural_networks/","page":"Neural networks","title":"Neural networks","text":"Using these bounds, the model can be compressed.","category":"page"},{"location":"neural_networks/","page":"Neural networks","title":"Neural networks","text":"compressed, removed = NN_compress(model, init_U, init_L, bounds_U, bounds_L)","category":"page"},{"location":"neural_networks/","page":"Neural networks","title":"Neural networks","text":"Compression can also be done without precomputed bounds.","category":"page"},{"location":"neural_networks/","page":"Neural networks","title":"Neural networks","text":"bounds_U, bounds_L = NN_formulate!(jump_model, model, init_U, init_L; bound_tightening=\"standard\", compress=true)","category":"page"},{"location":"neural_networks/","page":"Neural networks","title":"Neural networks","text":"Use the JuMP model to calculate a forward pass through the network (input at the center of the domain).","category":"page"},{"location":"neural_networks/","page":"Neural networks","title":"Neural networks","text":"forward_pass!(jump_model, [-1.0, 0.0])","category":"page"},{"location":"neural_networks/","page":"Neural networks","title":"Neural networks","text":"Finding the optimum of the neural network is now very straightforward.","category":"page"},{"location":"neural_networks/","page":"Neural networks","title":"Neural networks","text":"last_layer, _ = maximum(keys(jump_model[:x].data))\n@objective(jump_model, Max, jump_model[:x][last_layer, 1]) # maximize the output neuron\noptimize!(jump_model)\nvalue.(jump_model[:x][0, :]) # maximum\nobjective_value(jump_model) # neural network output at maximum","category":"page"},{"location":"neural_networks/#Convolutional-neural-networks","page":"Neural networks","title":"Convolutional neural networks","text":"","category":"section"},{"location":"neural_networks/","page":"Neural networks","title":"Neural networks","text":"The convolutional neural network requirements can be found in the CNN_formulate! documentation.","category":"page"},{"location":"neural_networks/","page":"Neural networks","title":"Neural networks","text":"First, create some kind of input (or load an image from your computer).","category":"page"},{"location":"neural_networks/","page":"Neural networks","title":"Neural networks","text":"input = rand(Float32, 70, 50, 1, 1) # BW 70x50 image","category":"page"},{"location":"neural_networks/","page":"Neural networks","title":"Neural networks","text":"Then, create a convolutional neural network model satisfying the requirements:","category":"page"},{"location":"neural_networks/","page":"Neural networks","title":"Neural networks","text":"using Flux\n\nCNN_model = Flux.Chain(\n    Conv((4,3), 1 => 10, pad=(2, 1), stride=(3, 2), relu),\n    MeanPool((5,3), pad=(3, 2), stride=(2, 2)),\n    MaxPool((3,4), pad=(1, 3), stride=(3, 2)),\n    Conv((4,3), 10 => 5, pad=(2, 1), stride=(3, 2), relu),\n    MaxPool((3,4), pad=(1, 3), stride=(3, 2)),\n    Flux.flatten,\n    Dense(20 => 100, relu),\n    Dense(100 => 1)\n)","category":"page"},{"location":"neural_networks/","page":"Neural networks","title":"Neural networks","text":"Then, create an empty JuMP model, extract the layer structure of the CNN model and finally formulate the MIP.","category":"page"},{"location":"neural_networks/","page":"Neural networks","title":"Neural networks","text":"jump = Model(Gurobi.Optimizer)\nset_silent(jump)\ncnns = get_structure(CNN_model, input);\nCNN_formulate!(jump, CNN_model, cnns)","category":"page"},{"location":"neural_networks/","page":"Neural networks","title":"Neural networks","text":"Check that the JuMP model produces the same outputs as the Flux.Chain.","category":"page"},{"location":"neural_networks/","page":"Neural networks","title":"Neural networks","text":"vec(CNN_model(input)) ≈ image_pass!(jump, input)","category":"page"},{"location":"neural_networks/#Bound-tightening","page":"Neural networks","title":"Bound tightening","text":"","category":"section"},{"location":"neural_networks/","page":"Neural networks","title":"Neural networks","text":"To improve the computational feasibility of the mixed-integer formulation of a neural network, the big-M values associated with the some of the constraints can be made smaller by calculating the minimum and and maximum activations of the individual neurons. This can be done with a heuristic algorithm or by using mathematical optimization.","category":"page"},{"location":"neural_networks/","page":"Neural networks","title":"Neural networks","text":"Our package includes three different modes of bound tightening: fast (default), standard and output.","category":"page"},{"location":"neural_networks/","page":"Neural networks","title":"Neural networks","text":"The fast mode uses a heuristic algorithm to determine the neuron activation bounds only based on the activation bounds of the previous layer. This algorithm practically doesn't increase the formulation time, so it is enabled by default. \nThe standard mode considers the whole mixed-integer problem with variables and constraints defined up to the previous layer from the neuron under bound tightening. It uses optimization to find the neuron activation bounds and is therefore significantly slower than the fast mode but is able to produce tighter bounds (smaller big-M values).\nIn some situations, the user might know the bounds for the output layer neurons. The output mode takes into account these output bounds as well as the whole MIP. Therefore, it is able to produce the tightest bounds of all the methods listed, but it is also the slowest.","category":"page"},{"location":"neural_networks/","page":"Neural networks","title":"Neural networks","text":"(precomputed is also one of the bound tightening options in the functions. It can be used by inputting bounds that have already been calculated.)","category":"page"},{"location":"neural_networks/","page":"Neural networks","title":"Neural networks","text":"A detailed discussion on bound tightening techniques can be found in Grimstad and Andresson (2019).","category":"page"},{"location":"neural_networks/#Sampling","page":"Neural networks","title":"Sampling","text":"","category":"section"},{"location":"neural_networks/","page":"Neural networks","title":"Neural networks","text":"Instead of just solving the MIP, the neural network can be optimized (finding the output maximizing/minimizing input) by using a sampling approach. Note that these features are experimental and cannot be guranteed to find the global optimum.","category":"page"},{"location":"neural_networks/","page":"Neural networks","title":"Neural networks","text":"using QuasiMonteCarlo\n\njump_model = Model(Gurobi.Optimizer)\nset_silent(jump_model)\nNN_formulate!(jump_model, NN_model, init_U, init_L; bound_tightening=\"fast\");\n\n# set objective function as the last layer output\nlast_layer, _ = maximum(keys(jump_model[:x].data))\n@objective(jump_model, Max, jump_model[:x][last_layer, 1])\n\nsamples = QuasiMonteCarlo.sample(1000, init_L, init_U, LatinHypercubeSample());\nx_opt, optimum = optimize_by_sampling!(jump_model, samples);","category":"page"},{"location":"neural_networks/#Relaxing-walk-algorithm","page":"Neural networks","title":"Relaxing walk algorithm","text":"","category":"section"},{"location":"neural_networks/","page":"Neural networks","title":"Neural networks","text":"Another method for heuristically optimizing the JuMP model is the so-called relaxing walk algorithm. It is based on a sampling approach that utilizes LP relaxations of the original problem and a pseudo gradient descent -algorithm.","category":"page"},{"location":"neural_networks/","page":"Neural networks","title":"Neural networks","text":"jump_model = Model(Gurobi.Optimizer)\nset_silent(jump_model)\nNN_formulate!(jump_model, NN_model, init_U, init_L; bound_tightening=\"fast\")\n\n# set objective function as the last layer output\nlast_layer, _ = maximum(keys(jump_model[:x].data))\n@objective(jump_model, Max, jump_model[:x][last_layer, 1])\n\nx_opt, optimum = optimize_by_walking!(jump_model, init_U, init_L)","category":"page"},{"location":"neural_networks/","page":"Neural networks","title":"Neural networks","text":"A set_solver! -function must be specified (used for copying the model in the algorithm).","category":"page"},{"location":"neural_networks/","page":"Neural networks","title":"Neural networks","text":"function set_solver!(jump)\n    set_optimizer(jump, Gurobi.Optimizer)\n    set_silent(jump)\nend","category":"page"},{"location":"neural_networks/#Recommendations","page":"Neural networks","title":"Recommendations","text":"","category":"section"},{"location":"neural_networks/","page":"Neural networks","title":"Neural networks","text":"The choice of the best neural network bound tightening and compression procedures depends heavily on your specific use case.  Based on some limited computational tests of our own as well as knowledge from the field, we can make the following general recommendations:","category":"page"},{"location":"neural_networks/","page":"Neural networks","title":"Neural networks","text":"Wide but shallow neural networks should be preferred. The bound tightening gets exponentially harder with deeper layers.\nFor small neural network models, using the \"fast\" bound tightening option is probably the best, since the resulting formulations are easy to solve even with loose bounds.\nFor larger neural networks, \"standard\" bound tightening will produce tighter bounds but take more time. However, when using the JuMP model, the tighter bounds might make it more computationally feasible.\nFor large neural networks where the output bounds are known, \"output\" bound tightening can be used. This bound tightening is very slow but might be necessary to increase the computational feasibility of the resulting JuMP model.\nIf the model has many so-called \"dead\" neurons, creating the JuMP model by using compression is beneficial, since the formulation will have fewer constraints and the bound tightening will be faster, reducing total formulation time.","category":"page"},{"location":"neural_networks/","page":"Neural networks","title":"Neural networks","text":"These are only general recommendations based on limited evidence, and the user should validate the performance of each bound tightening and compression procedure in relation to her own work.","category":"page"},{"location":"reference/","page":"Reference","title":"Reference","text":"CurrentModule = Gogeta","category":"page"},{"location":"reference/","page":"Reference","title":"Reference","text":"","category":"page"},{"location":"reference/","page":"Reference","title":"Reference","text":"Modules = [Gogeta]","category":"page"},{"location":"reference/#Gogeta.CNNStructure","page":"Reference","title":"Gogeta.CNNStructure","text":"struct CNNStructure\n\nContainer for the layer structure of a convolutional neural network.\n\nThis structure is used for passing the CNN parameters from one function to another. This structure can be created with the get_structure -function.\n\nFields\n\nchannels: dictionary of (layer, number of channels) pairs for convolutional or pooling layers\ndims: dictionary of (layer, (# of rows, # of columns)) pairs for convolutional or pooling layers\ndense_lengths: dictionary of (layer, # of neurons) pairs for dense and flatten layers\nconv_inds: vector of the convolutional layer indices\nmaxpool_inds_inds: vector of the maxpool layer indices\nmeanpool_inds: vector of the meanpool layer indices\nflatten_ind: index of the Flux.flatten layer\ndense_inds: vector of the dense layer indices\n\n```\n\n\n\n\n\n","category":"type"},{"location":"reference/#Gogeta.TEModel","page":"Reference","title":"Gogeta.TEModel","text":"struct TEModel\n\nUniversal datatype for storing information about a Tree Ensemble Model. This is the datatype that is used when creating the integer optimization problem from a tree ensemble.\n\nDifferent tree models (EvoTrees, XGBoost, RandomForest) require individual conversion functions to this datatype.\n\nFields\n\nn_trees: number of trees in the ensemble\nn_feats: number of features (input variables) in the model\nn_leaves: number of leaves on each tree\nleaves: indices of the leaves on each tree\nsplits: [feature, splitpoint index] pairs accessible by [tree, node]\nsplits_ordered: splitpoints ordered by split value for each feature\nn_splits: number of splitpoints for each feature\npredictions: prediction of each node (zero for nodes that are not leaves)\nsplit_nodes: boolean array containing information whether a node is a split node or not\n\nSplitpoints is the set of unique condition values from the ensemble. Each node is associated with a condition value.\n\n\n\n\n\n","category":"type"},{"location":"reference/#Gogeta.CNN_formulate!-Tuple{JuMP.Model, Flux.Chain, CNNStructure}","page":"Reference","title":"Gogeta.CNN_formulate!","text":"function create_MIP_from_CNN!(jump_model::JuMP.Model, CNN_model::Flux.Chain, cnnstruct::CNNStructure)\n\nCreates a mixed-integer optimization problem from a Flux.Chain convolutional neural network model. The optimization formulation is saved in the JuMP.Model given as an input.\n\nA dummy objective function of 1 is added to the model. The objective is left for the user to define.\n\nThe convolutional neural network must follow a certain structure:\n\nIt must consist of (in order) convolutional and pooling layers, a Flux.flatten layer and finally dense layers\nI.e. allowed layer types: Conv, MaxPool, MeanPool, Flux.flatten, Dense\nThe activation function for all of the convolutional layers and the dense layers must be ReLU\nThe last dense layer must use the identity activation function\nInput size, filter size, stride and padding can be chosen freely\n\nParameters\n\njump_model: an empty optimization model where the formulation will be saved\nCNN_model: Flux.Chain containing the CNN\ncnnstruct: holds the layer structure of the CNN\n\nOptional Parameters\n\nmax_to_mean: formulate maxpool layers as meanpool layers. This might improve the convex hull of the model (linear relaxation) for use with relaxing walk algorithm.\nlogging: print progress info to console.\n\n\n\n\n\n","category":"method"},{"location":"reference/#Gogeta.NN_compress-Tuple{Flux.Chain, Vararg{Any, 4}}","page":"Reference","title":"Gogeta.NN_compress","text":"function NN_compress(NN_model::Flux.Chain, U_in, L_in, U_bounds, L_bounds)\n\nCompresses a neural network using precomputed bounds.\n\nArguments\n\nNN_model: Neural network to be compressed.\nU_in: Upper bounds for the input variables.\nL_in: Lower bounds for the input variables.\nU_bounds: Upper bounds for the other neurons.\nL_bounds: Lower bounds for the other neurons.\n\nReturns a Flux.Chain model of the compressed neural network.\n\n\n\n\n\n","category":"method"},{"location":"reference/#Gogeta.NN_formulate!-Tuple{JuMP.Model, Flux.Chain, Any, Any}","page":"Reference","title":"Gogeta.NN_formulate!","text":"function NN_formulate!(jump_model::JuMP.Model, NN_model::Flux.Chain, U_in, L_in; U_bounds=nothing, L_bounds=nothing, U_out=nothing, L_out=nothing, bound_tightening=\"fast\", compress=false, parallel=false, silent=true)\n\nCreates a mixed-integer optimization problem from a Flux.Chain model.\n\nThe parameters are used to specify what kind of bound tightening and compression will be used.\n\nA dummy objective function of 1 is added to the model. The objective is left for the user to define.\n\nArguments\n\njump_model: The constraints and variables will be saved to this optimization model.\nNN_model: Neural network model to be formulated.\nU_in: Upper bounds for the input variables.\nL_in: Lower bounds for the input variables.\n\nOptional arguments\n\nbound_tightening: Mode selection: \"fast\", \"standard\", \"output\" or \"precomputed\"\ncompress: Should the model be simultaneously compressed?\nparallel: Runs bound tightening in parallel. set_solver!-function must be defined in the global scope, see documentation or examples.\nU_bounds: Upper bounds. Needed if bound_tightening=\"precomputed\"\nL_bounds: Lower bounds. Needed if bound_tightening=\"precomputed\"\nU_out: Upper bounds for the output variables. Needed if bound_tightening=\"output\".\nL_out: Lower bounds for the output variables. Needed if bound_tightening=\"output\".\nsilent: Controls console ouput.\n\n\n\n\n\n","category":"method"},{"location":"reference/#Gogeta.TE_formulate!-Tuple{JuMP.Model, TEModel, Any}","page":"Reference","title":"Gogeta.TE_formulate!","text":"function TE_formulate!(opt_model::JuMP.Model, TE::TEModel, objective)\n\nFormulates a tree ensemble to the JuMP model opt_model based on the given tree ensemble TE.\n\nThe JuMP model is formulated without the split constraints.\n\nAn objective function either minimizing or maximizing the ensemble output is added to the JuMP model.\n\nArguments\n\nopt_model: A JuMP model where the formulation will be saved to.\nTE: A tree ensemble model in the universal data type TEModel. \nobjective: MINSENSE or MAXSENSE. Minimize or maximize the tree ensemble output.\n\n\n\n\n\n","category":"method"},{"location":"reference/#Gogeta.add_split_constraints!-Tuple{JuMP.Model, TEModel}","page":"Reference","title":"Gogeta.add_split_constraints!","text":"function add_split_constraints!(opt_model::JuMP.Model, TE::TEModel)\n\nAdds all split constraints to the formulation.\n\nArguments\n\nopt_model: A JuMP model containing the formulation.\nTE: A tree ensemble model in the universal data type TEModel. \n\n\n\n\n\n","category":"method"},{"location":"reference/#Gogeta.build_model!-NTuple{4, Any}","page":"Reference","title":"Gogeta.build_model!","text":"function build_model!(W, b, K, neurons)\n\nBuilds a new Flux.Chain model from the given weights and biases. Modifies the W and b arrays.\n\nReturns the new Flux.Chain model.\n\n\n\n\n\n","category":"method"},{"location":"reference/#Gogeta.calculate_bounds-Tuple{JuMP.Model, Vararg{Any, 5}}","page":"Reference","title":"Gogeta.calculate_bounds","text":"function calculate_bounds(model::JuMP.Model, layer, neuron, W, b, neurons; layers_removed=0)\n\nCalculates the upper and lower activation bounds for a neuron in a ReLU-activated neural network.\n\n\n\n\n\n","category":"method"},{"location":"reference/#Gogeta.children-Tuple{Int64, Dict, Int64}","page":"Reference","title":"Gogeta.children","text":"function children(id::Int, leaf_dict::Dict, max::Int)\n\nFinds the leaf indices of the children leaves of node id in a binary tree.\n\nReturns an array of the leaf indices.\n\nArguments\n\nid: Index of the node in a binary tree. Indexing starts from one and follows level order.\nleaf_dict: A dictionary (map) of the leaf indices accessible by the node indices.\nmax: Biggest possible node id in the tree. Used to terminate the search.\n\n\n\n\n\n","category":"method"},{"location":"reference/#Gogeta.copy_model-Tuple{Any}","page":"Reference","title":"Gogeta.copy_model","text":"function copy_model(input_model, solver_params)\n\nCreates a copy of a JuMP model. Solver has to be specified for each new copy. Used for parallelization.\n\n\n\n\n\n","category":"method"},{"location":"reference/#Gogeta.extract_evotrees_info-Tuple{Any}","page":"Reference","title":"Gogeta.extract_evotrees_info","text":"extract_evotrees_info(evo_model; tree_limit=length(evo_model.trees))\n\nGets the data required for constructing the corresponding MIP from an EvoTrees model evo_model.  Returns a custom datatype TEModel which contains the necessary information.\n\nArguments\n\nevo_model: A trained EvoTrees tree ensemble model.\n\nOptional arguments\n\ntree_limit: only first n trees specified by the argument will be used\n\n\n\n\n\n","category":"method"},{"location":"reference/#Gogeta.forward_pass!-Tuple{JuMP.Model, Any}","page":"Reference","title":"Gogeta.forward_pass!","text":"function forward_pass!(jump_model::JuMP.Model, input)\n\nCalculates the output of a JuMP model representing a neural network.\n\n\n\n\n\n","category":"method"},{"location":"reference/#Gogeta.get_solution-Tuple{JuMP.Model, TEModel}","page":"Reference","title":"Gogeta.get_solution","text":"function get_solution(model::JuMP.Model, TE::TEModel)\n\nFinds the upper and lower bounds for each input variable given the optimized model.\n\nReturns the bounds for each feature in an array.\n\nArguments\n\nmodel: The optimized JuMP model.\nTE: Struct of type TEModel containing information about the tree ensemble.\n\n\n\n\n\n","category":"method"},{"location":"reference/#Gogeta.get_structure-Tuple{Flux.Chain, Array{Float32, 4}}","page":"Reference","title":"Gogeta.get_structure","text":"function get_structure(CNN_model::Flux.Chain, input::Array{Float32, 4})\n\nExtract the layer structure of a convolutional neural network. The input image is needed to calculate the correct sizes for the hidden 2-dimensional layers.\n\nReturns a CNNStructure struct.\n\n\n\n\n\n","category":"method"},{"location":"reference/#Gogeta.image_pass!-Tuple{JuMP.Model, Array{Float32, 4}, CNNStructure, Int64}","page":"Reference","title":"Gogeta.image_pass!","text":"function image_pass!(jump_model::JuMP.Model, input::Array{Float32, 4}, cnnstruct::CNNStructure, layer::Int)\n\nDebugging version\n\nForward pass an image through the JuMP model representing a convolutional neural network.\n\nReturns the output of the layer with index given as input.\n\n\n\n\n\n","category":"method"},{"location":"reference/#Gogeta.image_pass!-Tuple{JuMP.Model, Array{Float32, 4}}","page":"Reference","title":"Gogeta.image_pass!","text":"function image_pass!(jump_model::JuMP.Model, input::Array{Float32, 4})\n\nForward pass an image through the JuMP model representing a convolutional neural network.\n\nReturns the output of the network, i.e., a vector of the activations of the last dense layer neurons.\n\n\n\n\n\n","category":"method"},{"location":"reference/#Gogeta.init_TEModel!-Tuple{TEModel}","page":"Reference","title":"Gogeta.init_TEModel!","text":"function init_TEModel!(TE::TEModel)\n\nPrecompute child leaves which are needed for generating the split constraints. Changes child_leaves field of the TEModel.\n\n\n\n\n\n","category":"method"},{"location":"reference/#Gogeta.local_search-NTuple{4, Any}","page":"Reference","title":"Gogeta.local_search","text":"function local_search(start, jump_model, nn_model, U_in, L_in; max_iter=100, epsilon=0.01, show_path=false, logging=false, tolerance=0.001)\n\nPerforms relaxing walk local search on the given neural network JuMP formulation. See Tong et al. (2024) for more details.\n\nParameters\n\nstart: starting point for the search (coordinate in the space)\njump_model: JuMP model containing the NN formulation\n\nOptional Parameters\n\nepsilon: controls the step size taken out of the linear region\nshow_path: return the path taken by the local search in addition to the optimum\nlogging: print progress info to console\ntolerance: minimum relative improvement required at every step to continue the search\n\n\n\n\n\n","category":"method"},{"location":"reference/#Gogeta.local_search_CNN-Tuple{Any, Any}","page":"Reference","title":"Gogeta.local_search_CNN","text":"function local_search_CNN(start, cnn_jump; epsilon=0.01, max_iter=10, show_path=false, logging=false, tolerance=0.01)\n\nPerforms relaxing walk local search on the given convolutional neural network JuMP formulation. See Tong et al. (2024) for more details.\n\nParameters\n\nstart: starting point for the search (coordinate in the image space)\ncnn_jump: JuMP model containing the CNN formulation\n\nOptional Parameters\n\nepsilon: controls the step size taken out of the linear region\nmax_iter: maximum number of search steps\nshow_path: return the path taken by the local search in addition to the optimum\nlogging: print progress info to console\ntolerance: minimum relative improvement required at every step to continue the search\n\n\n\n\n\n","category":"method"},{"location":"reference/#Gogeta.optimize_by_sampling!-Tuple{JuMP.Model, Any}","page":"Reference","title":"Gogeta.optimize_by_sampling!","text":"function optimize_by_sampling!(jump_model, sample_points; enhanced=true, exploitation_rate=0.67)\n\nOptimizes a neural network by iteratively solving the \"local\" optimization problem at the sample points. The best (optimum, extremum) solution is returned.\n\nArguments\n\njump_model: JuMP model containing the formulation (and desired objective function)\nsample_points: A matrix where the columns are the sample inputs.\n\nOptional arguments\n\nenhanced: Controls whether local optimum or only local hyperplane corner is searched for.\nexploitation_rate: Controls how often the algorithm performs local search versus samples a new point.\n\n\n\n\n\n","category":"method"},{"location":"reference/#Gogeta.optimize_by_walking!-Tuple{JuMP.Model, Flux.Chain, Any, Any}","page":"Reference","title":"Gogeta.optimize_by_walking!","text":"function optimize_by_walking!(original::JuMP.Model, nn_model::Flux.Chain, U_in, L_in; delta=0.1, return_sampled=false, logging=true, iterations=10, infeasible_per_iter=5)\n\nPerforms the full relaxing walk algorithm on the given neural network JuMP formulation. See Tong et al. (2024) for more details.\n\nParameters\n\noriginal: JuMP model containing the NN formulation.\nnn_model: the original NN as Flux.Chain\n\nOptional Parameters\n\ndelta: controls how strongly certain neurons are preferred when fixing the binary variables\nreturn_sampled: return sampled points in addition to the optima \nlogging: print progress info to the console\niterations: the number of fresh starts from the linear relaxation (no binary variables fixed)\ninfeasible_per_iter: the number of infeasible LP relaxations allowed before starting next iteration\n\n\n\n\n\n","category":"method"},{"location":"reference/#Gogeta.optimize_by_walking_CNN!-Tuple{JuMP.Model, Any}","page":"Reference","title":"Gogeta.optimize_by_walking_CNN!","text":"function optimize_by_walking_CNN!(cnn_jump::JuMP.Model, input; iterations=10, samples_per_iter=5, timelimit=1.0, tolerance=1.0)\n\nPerforms the full relaxing walk algorithm on the given convolutional neural network JuMP formulation. See Tong et al. (2024) for more details.\n\nParameters\n\ncnn_jump: JuMP model containing the CNN formulation.\ninput: An example input to the CNN. Used to extract the input dimensions.\n\nOptional Parameters\n\niterations: the number of fresh starts from the linear relaxation (no binary variables fixed)\nsamples_per_iter: maximum number of successful samples per iteration\ntimelimit: time limit for the LP relaxation solves\ntolerance: how different new samples must be from any previous one\n\n\n\n\n\n","category":"method"},{"location":"reference/#Gogeta.prune!-NTuple{8, Any}","page":"Reference","title":"Gogeta.prune!","text":"function prune!(W, b, removed_neurons, layers_removed, neuron_count, layer, bounds_U, bounds_L)\n\nRemoves stabily active or inactive neurons in a network by updating the weights and the biases and the removed neurons list accordingly.\n\n\n\n\n\n","category":"method"},{"location":"reference/#Gogeta.tree_callback_algorithm-Tuple{Any, TEModel, JuMP.Model}","page":"Reference","title":"Gogeta.tree_callback_algorithm","text":"function tree_callback_algorithm(cb_data, TE::TEModel, opt_model::JuMP.Model)\n\nThe callback algorithm for tree ensemble optimization using lazy constraints.\n\nUsing lazy constraints, the split constraints are added one-by-one for each tree.\n\nSee examples or documentation for information on how to use lazy constraints.\n\nArguments\n\ncb_data: Callback data\nTE: A tree ensemble model in the universal data type TEModel. \nopt_model: A JuMP model containing the formulation.\n\n\n\n\n\n","category":"method"},{"location":"#Gogeta.jl","page":"Introduction","title":"Gogeta.jl","text":"","category":"section"},{"location":"","page":"Introduction","title":"Introduction","text":"Gogeta is a package that enables the user to formulate trained machine learning models as mathematical optimization problems.","category":"page"},{"location":"","page":"Introduction","title":"Introduction","text":"Currently supported models are Flux.Chain ReLU-activated neural networks (dense and convolutional) and EvoTrees tree ensemble models.","category":"page"},{"location":"#Installation","page":"Introduction","title":"Installation","text":"","category":"section"},{"location":"","page":"Introduction","title":"Introduction","text":"julia> Pkg.add(\"Gogeta\")","category":"page"},{"location":"#How-can-this-package-be-used?","page":"Introduction","title":"How can this package be used?","text":"","category":"section"},{"location":"","page":"Introduction","title":"Introduction","text":"Formulating trained machine learning (ML) models as mixed-integer programming (MIP) problems opens up multiple possibilities. Firstly, it allows for global optimization - finding the input that provably maximizes or minimizes the ML model output. Secondly, changing the objective function in the MIP formulation and/or adding additional constraints makes it possible to solve problems related to the ML model, such as finding adversarial inputs. Lastly, the MIP formulation of a ML model can be included into a larger optimization problem. This is useful in surrogate contexts where an ML model can be trained to approximate a complicated function that itself cannot be used in an optimization problem.","category":"page"},{"location":"","page":"Introduction","title":"Introduction","text":"Despite its usefulness, modeling ML models as MIP problems has significant limitations. The biggest limitation is the capability of MIP solvers which limits the ML model size.  With neural networks, for example, only models with at most hundreds of neurons can be effectively tackled. In practice, formulating into MIPs and optimizing all large modern models such as convolutional neural networks and transformer networks is computationally infeasible. However, if small neural networks are all that is required for the specific application, the techniques implemented in this package can be useful. Secondly, only piecewise linear ML models can be formulated as MIP problems. For example, with neural networks this entails using only ReLU as the activation function.","category":"page"},{"location":"#Getting-started","page":"Introduction","title":"Getting started","text":"","category":"section"},{"location":"","page":"Introduction","title":"Introduction","text":"The following sections Tree ensembles and Neural networks give a very simple demonstration on how to use the package.  Multiprocessing examples and more detailed code can be found in the examples/-folder of the package repository.","category":"page"},{"location":"literature/#Literature","page":"Literature","title":"Literature","text":"","category":"section"},{"location":"literature/","page":"Literature","title":"Literature","text":"The mathematical optimization methods implemented in this package are based on the work of many brilliant researchers. The most important papers for our work are listed here. In these works, more in-depth information about the formulations and various algorithms we use can also be found.","category":"page"},{"location":"literature/","page":"Literature","title":"Literature","text":"(Convolutional) neural network formulation:\nFischetti, M., & Jo, J. (2018). Deep neural networks and mixed integer linear optimization. Constraints, 23(3), 296-309.\nNeural network compression:\nSerra, T., Kumar, A., & Ramalingam, S. (2020, September). Lossless compression of deep neural networks. In International conference on integration of constraint programming, artificial intelligence, and operations research (pp. 417-430). Cham: Springer International Publishing.\nToivonen, V. (2024). Lossless Compression of Deep Neural Networks.\nBound tightening techniques:\nGrimstad, B., & Andersson, H. (2019). ReLU networks as surrogate models in mixed-integer linear programs. Computers & Chemical Engineering, 131, 106580.\nSampling-based optimization:\nPerakis, G., & Tsiourvas, A. (2022). Optimizing Objective Functions from Trained ReLU Neural Networks via Sampling. arXiv preprint arXiv:2205.14189.\nTong, J., Cai, J., & Serra, T. (2024). Optimization Over Trained Neural Networks: Taking a Relaxing Walk. arXiv preprint arXiv:2401.03451.\nTree ensembles:\nMišić, V. V. (2020). Optimization of tree ensembles. Operations Research, 68(5), 1605-1624.","category":"page"}]
}
