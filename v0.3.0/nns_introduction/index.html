<!DOCTYPE html>
<html lang="en"><head><meta charset="UTF-8"/><meta name="viewport" content="width=device-width, initial-scale=1.0"/><title>General · Gogeta.jl</title><meta name="title" content="General · Gogeta.jl"/><meta property="og:title" content="General · Gogeta.jl"/><meta property="twitter:title" content="General · Gogeta.jl"/><meta name="description" content="Documentation for Gogeta.jl."/><meta property="og:description" content="Documentation for Gogeta.jl."/><meta property="twitter:description" content="Documentation for Gogeta.jl."/><script data-outdated-warner src="../assets/warner.js"></script><link href="https://cdnjs.cloudflare.com/ajax/libs/lato-font/3.0.0/css/lato-font.min.css" rel="stylesheet" type="text/css"/><link href="https://cdnjs.cloudflare.com/ajax/libs/juliamono/0.050/juliamono.min.css" rel="stylesheet" type="text/css"/><link href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/6.4.2/css/fontawesome.min.css" rel="stylesheet" type="text/css"/><link href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/6.4.2/css/solid.min.css" rel="stylesheet" type="text/css"/><link href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/6.4.2/css/brands.min.css" rel="stylesheet" type="text/css"/><link href="https://cdnjs.cloudflare.com/ajax/libs/KaTeX/0.16.8/katex.min.css" rel="stylesheet" type="text/css"/><script>documenterBaseURL=".."</script><script src="https://cdnjs.cloudflare.com/ajax/libs/require.js/2.3.6/require.min.js" data-main="../assets/documenter.js"></script><script src="../search_index.js"></script><script src="../siteinfo.js"></script><script src="../../versions.js"></script><link class="docs-theme-link" rel="stylesheet" type="text/css" href="../assets/themes/catppuccin-mocha.css" data-theme-name="catppuccin-mocha"/><link class="docs-theme-link" rel="stylesheet" type="text/css" href="../assets/themes/catppuccin-macchiato.css" data-theme-name="catppuccin-macchiato"/><link class="docs-theme-link" rel="stylesheet" type="text/css" href="../assets/themes/catppuccin-frappe.css" data-theme-name="catppuccin-frappe"/><link class="docs-theme-link" rel="stylesheet" type="text/css" href="../assets/themes/catppuccin-latte.css" data-theme-name="catppuccin-latte"/><link class="docs-theme-link" rel="stylesheet" type="text/css" href="../assets/themes/documenter-dark.css" data-theme-name="documenter-dark" data-theme-primary-dark/><link class="docs-theme-link" rel="stylesheet" type="text/css" href="../assets/themes/documenter-light.css" data-theme-name="documenter-light" data-theme-primary/><script src="../assets/themeswap.js"></script></head><body><div id="documenter"><nav class="docs-sidebar"><div class="docs-package-name"><span class="docs-autofit"><a href="../">Gogeta.jl</a></span></div><button class="docs-search-query input is-rounded is-small is-clickable my-2 mx-auto py-1 px-2" id="documenter-search-query">Search docs (Ctrl + /)</button><ul class="docs-menu"><li><a class="tocitem" href="../">Introduction</a></li><li><span class="tocitem">Features</span><ul><li><input class="collapse-toggle" id="menuitem-2-1" type="checkbox" checked/><label class="tocitem" for="menuitem-2-1"><span class="docs-label">Neural networks</span><i class="docs-chevron"></i></label><ul class="collapsed"><li class="is-active"><a class="tocitem" href>General</a><ul class="internal"><li><a class="tocitem" href="#Requirements-for-the-architecture-of-the-NN"><span>Requirements for the architecture of the NN</span></a></li><li><a class="tocitem" href="#Bound-tightening-options"><span>Bound-tightening options</span></a></li><li><a class="tocitem" href="#Recommendations"><span>Recommendations</span></a></li></ul></li><li><a class="tocitem" href="../neural_networks/">Big-M formulation</a></li><li><a class="tocitem" href="../psplit_nns/">Partition-based formulation</a></li><li><a class="tocitem" href="../optimization/">Optimization</a></li><li><a class="tocitem" href="../nns_in_larger/">Use as surrogates</a></li></ul></li><li><a class="tocitem" href="../icnns/">Input convex neural networks</a></li><li><a class="tocitem" href="../cnns/">Convolutional neural networks</a></li><li><a class="tocitem" href="../tree_ensembles/">Tree ensembles</a></li></ul></li><li><a class="tocitem" href="../literature/">Literature</a></li><li><a class="tocitem" href="../reference/">Reference</a></li></ul><div class="docs-version-selector field has-addons"><div class="control"><span class="docs-label button is-static is-size-7">Version</span></div><div class="docs-selector control is-expanded"><div class="select is-fullwidth is-size-7"><select id="documenter-version-selector"></select></div></div></div></nav><div class="docs-main"><header class="docs-navbar"><a class="docs-sidebar-button docs-navbar-link fa-solid fa-bars is-hidden-desktop" id="documenter-sidebar-button" href="#"></a><nav class="breadcrumb"><ul class="is-hidden-mobile"><li><a class="is-disabled">Features</a></li><li><a class="is-disabled">Neural networks</a></li><li class="is-active"><a href>General</a></li></ul><ul class="is-hidden-tablet"><li class="is-active"><a href>General</a></li></ul></nav><div class="docs-right"><a class="docs-navbar-link" href="https://github.com/gamma-opt/Gogeta.jl" title="View the repository on GitHub"><span class="docs-icon fa-brands"></span><span class="docs-label is-hidden-touch">GitHub</span></a><a class="docs-navbar-link" href="https://github.com/gamma-opt/Gogeta.jl/blob/main/docs/src/nns_introduction.md" title="Edit source on GitHub"><span class="docs-icon fa-solid"></span></a><a class="docs-settings-button docs-navbar-link fa-solid fa-gear" id="documenter-settings-button" href="#" title="Settings"></a><a class="docs-article-toggle-button fa-solid fa-chevron-up" id="documenter-article-toggle-button" href="javascript:;" title="Collapse all docstrings"></a></div></header><article class="content" id="documenter-page"><h1 id="Neural-Networks"><a class="docs-heading-anchor" href="#Neural-Networks">Neural Networks</a><a id="Neural-Networks-1"></a><a class="docs-heading-anchor-permalink" href="#Neural-Networks" title="Permalink"></a></h1><p>With the <code>Gogeta</code> package, it is currently possible to formulate deep neural networks (NNs) and convolutional neural networks (CNNs) as mixed integer problems (MIP). We start the discussion from NNs. More detailed code can be found in the <a href="https://github.com/gamma-opt/Gogeta.jl/tree/main/examples">example jupyter notebooks</a>.</p><p>Currently, there are two different ways how to formulate <code>Flux.Chain</code> neural network models as MILPs: using the <a href="../neural_networks/">big-M approach</a> or the <a href="../psplit_nns/">Psplits</a> formulation. In this section, we are going to introduce some concepts related to both of them. </p><div class="admonition is-info"><header class="admonition-header">Note</header><div class="admonition-body"><p>This and following sections describe how to formulate a <code>Flux.Chain</code> neural network model as a MILP. If you want to use neural networks as surrogate models in a larger optimization problem, <a href="../nns_in_larger/">this section</a> has a guide on how to accomplish this effectively and formulate the neural network with anonymous variables.</p></div></div><h2 id="Requirements-for-the-architecture-of-the-NN"><a class="docs-heading-anchor" href="#Requirements-for-the-architecture-of-the-NN">Requirements for the architecture of the NN</a><a id="Requirements-for-the-architecture-of-the-NN-1"></a><a class="docs-heading-anchor-permalink" href="#Requirements-for-the-architecture-of-the-NN" title="Permalink"></a></h2><p>In order to formulate NNs as MIPs, they must use the <span>$ReLU$</span> activation function at each hidden layer, and the output layer must use the identity activation function. See the example below:</p><pre><code class="language-julia hljs">using Flux

NN_model = Chain(
    Dense(2 =&gt; 10, relu),
    Dense(10 =&gt; 20, relu),
    Dense(20 =&gt; 5, relu),
    Dense(5 =&gt; 1)
)</code></pre><p>For each input variable, upper and lower bounds need to be provided. The formulation will ensure to produce the same output as NN in these ranges.</p><pre><code class="language-julia hljs">init_U = [-0.5, 0.5];
init_L = [-1.5, -0.5];</code></pre><p>A neural network satifying these requirements can be formulated into a mixed-integer linear optimization problem (MILP).  Along with formulation, the neuron activation bounds can be calculated, which improves computational performance as well as enables compression.</p><h2 id="Bound-tightening-options"><a class="docs-heading-anchor" href="#Bound-tightening-options">Bound-tightening options</a><a id="Bound-tightening-options-1"></a><a class="docs-heading-anchor-permalink" href="#Bound-tightening-options" title="Permalink"></a></h2><p>The formulations require calculating boundary values for each neuron. <code>bound_tightening</code> refers to the way how the boundaries are calculated.</p><p>The <em>(default)</em> way to do this is called <code>fast</code>, but you may also use <code>standard</code>,<code>precomputed</code>, <code>output</code>.</p><ol><li>The <code>fast</code> mode uses a heuristic algorithm to determine the neuron activation bounds only based on the activation bounds of the previous layer. This algorithm practically doesn&#39;t increase the formulation time, so it is enabled by default. </li><li>The <code>standard</code> mode considers the whole mixed-integer problem with variables and constraints defined up to the previous layer from the neuron under bound tightening. It uses optimization to find the neuron activation bounds and is therefore significantly slower than the <code>fast</code> mode but is able to produce tighter bounds (smaller big-M values).</li><li>In some situations, the user might know the bounds for the output layer neurons. The <code>output</code> mode takes into account these output bounds as well as the whole MIP. Therefore, it is able to produce the tightest bounds of all the methods listed, but it is also the slowest.</li><li><code>precomputed</code> is the last of the bound tightening options. It can be used to input bounds that have already been calculated.</li></ol><p>A detailed discussion on bound tightening techniques can be found in <a href="../literature/">Grimstad and Andresson (2019)</a>.</p><h2 id="Recommendations"><a class="docs-heading-anchor" href="#Recommendations">Recommendations</a><a id="Recommendations-1"></a><a class="docs-heading-anchor-permalink" href="#Recommendations" title="Permalink"></a></h2><p>The choice of the best neural network bound tightening and compression procedures depends heavily on your specific use case.  Based on some limited computational tests of our own as well as knowledge from the field, we can make the following general recommendations:</p><ul><li>Wide but shallow neural networks should be preferred. The bound tightening gets exponentially harder with deeper layers.</li><li>For small neural network models, using the <code>fast</code> bound tightening option is probably the best, since the resulting formulations are easy to solve even with loose bounds.</li><li>For larger neural networks, <code>standard</code> bound tightening will produce tighter bounds but take more time. However, when using the <code>JuMP</code> model, the tighter bounds might make it more computationally feasible.</li><li>For large neural networks where the output bounds are known, <code>output</code> bound tightening can be used. This bound tightening is very slow but might be necessary to increase the computational feasibility of the resulting <code>JuMP</code> model.</li><li>If the model has many so-called &quot;dead&quot; neurons, creating the <code>JuMP</code> model by using compression is beneficial since the formulation will have fewer constraints and the bound tightening will be faster, reducing total formulation time.</li><li>With the partition-based formulation, choose a number of partitions wisely, since it greatly increases size of the MIP problem. You are interested in chosing the minimum number of partitions that result in the tightest bounds for the neurons.</li></ul><p>These are only general recommendations based on limited evidence, and the user should validate the performance of each bound tightening and compression procedure in relation to her own work.</p></article><nav class="docs-footer"><a class="docs-footer-prevpage" href="../">« Introduction</a><a class="docs-footer-nextpage" href="../neural_networks/">Big-M formulation »</a><div class="flexbox-break"></div><p class="footer-message">Powered by <a href="https://github.com/JuliaDocs/Documenter.jl">Documenter.jl</a> and the <a href="https://julialang.org/">Julia Programming Language</a>.</p></nav></div><div class="modal" id="documenter-settings"><div class="modal-background"></div><div class="modal-card"><header class="modal-card-head"><p class="modal-card-title">Settings</p><button class="delete"></button></header><section class="modal-card-body"><p><label class="label">Theme</label><div class="select"><select id="documenter-themepicker"><option value="auto">Automatic (OS)</option><option value="documenter-light">documenter-light</option><option value="documenter-dark">documenter-dark</option><option value="catppuccin-latte">catppuccin-latte</option><option value="catppuccin-frappe">catppuccin-frappe</option><option value="catppuccin-macchiato">catppuccin-macchiato</option><option value="catppuccin-mocha">catppuccin-mocha</option></select></div></p><hr/><p>This document was generated with <a href="https://github.com/JuliaDocs/Documenter.jl">Documenter.jl</a> version 1.6.0 on <span class="colophon-date" title="Wednesday 21 August 2024 12:13">Wednesday 21 August 2024</span>. Using Julia version 1.9.4.</p></section><footer class="modal-card-foot"></footer></div></div></div></body></html>
